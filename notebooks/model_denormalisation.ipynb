{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19667d7e-1077-4398-937d-ca14391639ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reverse the z score normalisation of the nanopore pore models. HAs user tunable STD_DEV and MEAN values - which creates a current value for each kmer in the 9mer_levels_v1.txt file. Then creates squiggle from the reads found in testing_r10_fasta.fasta in the fasta directory, starts a guppy server instance and basecalles them, Then maps them to the chromsome 20 sequence in the fasta directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63e60367-bce4-4cc7-bf9d-488ddeded1e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking MEAN: 97, STD_DEV: 22\n",
      "Number of reads base called: 50 for Mean: 97, std dev 22\n",
      "Number of reads aligned: 45 for Mean: 97, std dev 22\n",
      "\n",
      "0.8818418766290183\n",
      "Checking MEAN: 97, STD_DEV: 23\n",
      "Number of reads base called: 49 for Mean: 97, std dev 23\n",
      "Number of reads aligned: 44 for Mean: 97, std dev 23\n",
      "\n",
      "0.8789503276157729\n",
      "Checking MEAN: 97, STD_DEV: 24\n",
      "Number of reads base called: 50 for Mean: 97, std dev 24\n",
      "Number of reads aligned: 45 for Mean: 97, std dev 24\n",
      "\n",
      "0.8809730668983493\n",
      "Checking MEAN: 98, STD_DEV: 22\n",
      "Number of reads base called: 50 for Mean: 98, std dev 22\n",
      "Number of reads aligned: 45 for Mean: 98, std dev 22\n",
      "\n",
      "0.8805166846071044\n",
      "Checking MEAN: 98, STD_DEV: 23\n",
      "Number of reads base called: 50 for Mean: 98, std dev 23\n",
      "Number of reads aligned: 45 for Mean: 98, std dev 23\n",
      "\n",
      "0.8810728744939271\n",
      "Checking MEAN: 98, STD_DEV: 24\n",
      "Number of reads base called: 50 for Mean: 98, std dev 24\n",
      "Number of reads aligned: 45 for Mean: 98, std dev 24\n",
      "\n",
      "0.8810763888888888\n",
      "Checking MEAN: 99, STD_DEV: 22\n",
      "Number of reads base called: 50 for Mean: 99, std dev 22\n",
      "Number of reads aligned: 45 for Mean: 99, std dev 22\n",
      "\n",
      "0.8822055137844611\n",
      "Checking MEAN: 99, STD_DEV: 23\n",
      "Number of reads base called: 49 for Mean: 99, std dev 23\n",
      "Number of reads aligned: 44 for Mean: 99, std dev 23\n",
      "\n",
      "0.8820306510962335\n",
      "Checking MEAN: 99, STD_DEV: 24\n",
      "Number of reads base called: 50 for Mean: 99, std dev 24\n",
      "Number of reads aligned: 45 for Mean: 99, std dev 24\n",
      "\n",
      "0.8812531581606872\n",
      "Checking MEAN: 100, STD_DEV: 22\n",
      "Number of reads base called: 50 for Mean: 100, std dev 22\n",
      "Number of reads aligned: 45 for Mean: 100, std dev 22\n",
      "\n",
      "0.8801463860933212\n",
      "Checking MEAN: 100, STD_DEV: 23\n",
      "Number of reads base called: 49 for Mean: 100, std dev 23\n",
      "Number of reads aligned: 44 for Mean: 100, std dev 23\n",
      "\n",
      "0.8802981321839081\n",
      "Checking MEAN: 100, STD_DEV: 24\n",
      "Number of reads base called: 50 for Mean: 100, std dev 24\n",
      "Number of reads aligned: 45 for Mean: 100, std dev 24\n",
      "\n",
      "0.8817733990147784\n",
      "Checking MEAN: 101, STD_DEV: 22\n",
      "Number of reads base called: 50 for Mean: 101, std dev 22\n",
      "Number of reads aligned: 45 for Mean: 101, std dev 22\n",
      "\n",
      "0.8794449262792715\n",
      "Checking MEAN: 101, STD_DEV: 23\n",
      "Number of reads base called: 50 for Mean: 101, std dev 23\n",
      "Number of reads aligned: 45 for Mean: 101, std dev 23\n",
      "\n",
      "0.8807069219440353\n",
      "Checking MEAN: 101, STD_DEV: 24\n",
      "Number of reads base called: 50 for Mean: 101, std dev 24\n",
      "Number of reads aligned: 45 for Mean: 101, std dev 24\n",
      "\n",
      "0.8806038426349497\n",
      "Checking MEAN: 102, STD_DEV: 22\n",
      "Number of reads base called: 49 for Mean: 102, std dev 22\n",
      "Number of reads aligned: 44 for Mean: 102, std dev 22\n",
      "\n",
      "0.8777874759016316\n",
      "Checking MEAN: 102, STD_DEV: 23\n",
      "Number of reads base called: 49 for Mean: 102, std dev 23\n",
      "Number of reads aligned: 44 for Mean: 102, std dev 23\n",
      "\n",
      "0.8813214821160849\n",
      "Checking MEAN: 102, STD_DEV: 24\n",
      "Number of reads base called: 49 for Mean: 102, std dev 24\n",
      "Number of reads aligned: 44 for Mean: 102, std dev 24\n",
      "\n",
      "0.8779485118069574\n",
      "[(0.8822055137844611, 99, 22), (0.8820306510962335, 99, 23), (0.8818418766290183, 97, 22), (0.8817733990147784, 100, 24), (0.8813214821160849, 102, 23), (0.8812531581606872, 99, 24), (0.8810763888888888, 98, 24), (0.8810728744939271, 98, 23), (0.8809730668983493, 97, 24), (0.8807069219440353, 101, 23), (0.8806038426349497, 101, 24), (0.8805166846071044, 98, 22), (0.8802981321839081, 100, 23), (0.8801463860933212, 100, 22), (0.8794449262792715, 101, 22), (0.8789503276157729, 97, 23), (0.8779485118069574, 102, 24), (0.8777874759016316, 102, 22)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "import time as t\n",
    "from contextlib import contextmanager, redirect_stdout\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import mappy as mp\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "from pyfastx import Fasta\n",
    "from pyguppy_client_lib import helper_functions\n",
    "from pyguppy_client_lib.pyclient import PyGuppyClient\n",
    "\n",
    "MEAN = 98\n",
    "STD_DEV = 23\n",
    "BIN_PATH = \"/usr/bin/\"\n",
    "FORMAT = \"%d%m%y%H%M%S\"\n",
    "DIGITISATION = 1\n",
    "RANGE = 1\n",
    "PREFIX = (\n",
    "    \"TTTTTTTTTTTTTTTTTTAATCAAGCAGCGGAGTTGAGGACGCGAGACGGGACTTTTTTAGCAGACTTTACGGACTACGACT\"\n",
    ")\n",
    "df = pd.read_csv(\"9mer_levels_v1.txt\", header=None, sep=\"\\t\", names=[\"kmer\", \"value\"])\n",
    "\n",
    "\n",
    "def pack(read: dict[str : Union[str, npt.NDArray[np.int16]]]):\n",
    "    \"\"\"Pack an ont_fast5_api.Fast5Read for calling\n",
    "    passed dict has two fields, read_id and raw_Data\n",
    "    read_id: str, raw_data: npt.NDArray[np.int16]\n",
    "    \"\"\"\n",
    "    read_id = read[\"read_id\"]\n",
    "    raw_data = read[\"raw_data\"]\n",
    "    scaling = RANGE / DIGITISATION\n",
    "    offset = 0.0\n",
    "    return helper_functions.package_read(read_id, raw_data, offset, scaling)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def start_guppy_server_and_client(bin_path, config, port, server_args):\n",
    "    server_args.extend(\n",
    "        [\"--config\", config, \"--port\", port, \"--log_path\", str((Path(\".\") / \"junk\"))]\n",
    "    )\n",
    "    # This function has it's own prints that may want to be suppressed\n",
    "    with redirect_stdout(StringIO()) as fh:\n",
    "        server, port = helper_functions.run_server(server_args, bin_path=bin_path)\n",
    "\n",
    "    if port == \"ERROR\":\n",
    "        raise RuntimeError(\"Server couldn't be started\")\n",
    "\n",
    "    if port.startswith(\"ipc\"):\n",
    "        address = f\"{port}\"\n",
    "    else:\n",
    "        address = f\"localhost:{port}\"\n",
    "    client = PyGuppyClient(address=address, config=config)\n",
    "\n",
    "    try:\n",
    "        with client:\n",
    "            yield client\n",
    "    finally:\n",
    "        server.terminate()\n",
    "\n",
    "\n",
    "def sliding_window(iterable, n):\n",
    "    # sliding_window('ABCDEFG', 4) --> ABCD BCDE CDEF DEFG\n",
    "    it = iter(iterable)\n",
    "    window = collections.deque(islice(it, n), maxlen=n)\n",
    "    if len(window) == n:\n",
    "        yield tuple(window)\n",
    "    for x in it:\n",
    "        window.append(x)\n",
    "        yield tuple(window)\n",
    "\n",
    "\n",
    "def signalify(\n",
    "    kmers: dict[str, dict[str, float]], sequence: str\n",
    ") -> npt.NDArray[np.int16]:\n",
    "    \"\"\"convert a given sequence to signal using R10 models,\n",
    "    returning np array containing 10 samples per base\"\"\"\n",
    "    a = []\n",
    "    # Always upper case signal\n",
    "    for kmer in sliding_window(sequence.upper(), 9):\n",
    "        value = kmers[\"\".join(kmer)][\"new_value\"]\n",
    "        value = (value * 2048) / 200 - 0\n",
    "        for _ in range(10):\n",
    "            a.append(value)\n",
    "    return np.array(a).astype(np.int16)\n",
    "\n",
    "\n",
    "def calculate_identity(cigar: str, goal_length: int):\n",
    "    match = re.findall(r\"(\\d+)([MIDNSHP=X])\", cigar)\n",
    "\n",
    "    # Calculate total matches, mismatches, and alignment length\n",
    "    total_matches = sum(int(length) for length, operation in match if operation == \"M\")\n",
    "    alignment_length = sum(\n",
    "        int(length) for length, operation in match if operation in [\"M\", \"D\", \"I\", \"N\"]\n",
    "    )\n",
    "    total_insertions = sum(\n",
    "        int(length) for length, operation in match if operation == \"I\"\n",
    "    )\n",
    "\n",
    "    # Calculate total query length (alignment length + insertions)\n",
    "    total_query_length = goal_length + total_insertions\n",
    "\n",
    "    # Identity is calculated as the number of matches divided by total query length\n",
    "    identity = total_matches / total_query_length if total_query_length > 0 else 0\n",
    "    return identity\n",
    "\n",
    "\n",
    "median_identities = []\n",
    "for MEAN in range(97, 103):\n",
    "    for STD_DEV in range(22, 25):\n",
    "        print(f\"Checking MEAN: {MEAN}, STD_DEV: {STD_DEV}\")\n",
    "        time = datetime.now().strftime(FORMAT)\n",
    "        work_dir = Path(\"working\") / time\n",
    "        work_dir.mkdir(exist_ok=True, parents=False)\n",
    "        df[\"new_value\"] = df[\"value\"] * STD_DEV + MEAN\n",
    "\n",
    "        kmer_values = df.set_index(\"kmer\").to_dict(orient=\"index\")\n",
    "\n",
    "        df.to_csv(f\"working/{time}/model_test.tsv\", sep=\"\\t\", index=None, header=None)\n",
    "\n",
    "        prefix = signalify(kmer_values, PREFIX)\n",
    "\n",
    "        with start_guppy_server_and_client(\n",
    "            BIN_PATH,\n",
    "            \"dna_r10.4.1_e8.2_400bps_hac.cfg\",\n",
    "            \"ipc:///tmp/.guppy/5555\",\n",
    "            [\"--device\", \"cuda:all\"],\n",
    "        ) as client:\n",
    "            fa = Fasta(\"../fasta/identity_fasta.fasta\")\n",
    "            for seq in fa:\n",
    "                # remove any non ACGT and replace with A\n",
    "                # sequence = re.sub(r\"[^ACGT]\", \"A\", seq.seq, flags=re.IGNORECASE)\n",
    "                signal = signalify(kmer_values, seq.seq)\n",
    "                signal = np.concatenate((prefix, signal))\n",
    "                read = {\"read_id\": seq.name, \"raw_data\": signal}\n",
    "                success_pass = client.pass_read(read, pack)\n",
    "            t.sleep(15)\n",
    "            res = client.get_completed_reads()\n",
    "            print(\n",
    "                f\"Number of reads base called: {len(res)} for Mean: {MEAN}, std dev {STD_DEV}\"\n",
    "            )\n",
    "            identities = []\n",
    "            al_count = 0\n",
    "            for r in res:\n",
    "                # print(r[0][\"datasets\"].get(\"sequence\", \"\"))\n",
    "                read_id = r[0][\"metadata\"][\"read_id\"]\n",
    "                reference_fasta = fa[read_id]\n",
    "                aligner = mp.Aligner(seq=str(reference_fasta), preset=\"map-ont\")\n",
    "                # print(bool(aligner))\n",
    "                start, end = r[0][\"metadata\"][\"read_id\"].rsplit(\"_\", 2)[-2:]\n",
    "                read_number = r[0][\"metadata\"][\"read_id\"].rsplit(\"_\", 3)[-3]\n",
    "                sequence = r[0][\"datasets\"].get(\"sequence\", \"\")\n",
    "                als = aligner.map(sequence)\n",
    "                for al in als:\n",
    "                    identity = calculate_identity(al.cigar_str, al.ctg_len)\n",
    "                    identities.append(identity)\n",
    "                    al_count += 1\n",
    "            print(\n",
    "                f\"Number of reads aligned: {al_count} for Mean: {MEAN}, std dev {STD_DEV}\\n\"\n",
    "            )\n",
    "        if identities:\n",
    "            m_id = np.median(identities)\n",
    "            median_identities.append((m_id, MEAN, STD_DEV))\n",
    "            print(m_id)\n",
    "print(sorted(median_identities, key=lambda x: x[0], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3657430-e2c6-4890-ba0a-fd26eb997f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
